{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Sequence to Sequence Learning with Neural Network\n",
    "In this notebook we are implementing Sequence to Sequence Learning with Neural Networks paper. https://arxiv.org/abs/1409.3215\n",
    "\n",
    "Reference: https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random, math, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "torch.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic= True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has model for each language (\"de\" for German and \"en\" for English) which need to be loaded so we can access the tokenizer of each model.\n",
    "\n",
    "Note: the models must first be downloaded using the following on the command line:\n",
    "\n",
    "python -m spacy download en\n",
    "\n",
    "python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de=spacy.load('de')\n",
    "spacy_en=spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper we are implementing, they find it beneficial to reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchText's Fields handle how data should be processed. You can read all of the possible arguments here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_de,\n",
    "            init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           lower=True)\n",
    "TRG=Field(tokenize=tokenizer_en,\n",
    "         init_token='<sos>',\n",
    "         eos_token='<eos>',\n",
    "         lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data= Multi30k.splits(exts=('.de','.en'),fields=(SRC,TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data,min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "train_iterator, valid_iterator, test_iterator =BucketIterator.splits((train_data, valid_data, test_data),batch_size=BATCH_SIZE,\n",
    "                                                                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.BucketIterator at 0x7fcb673cef98>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim=input_dim\n",
    "        self.emb_dim=emb_dim\n",
    "        self.hid_dim=hid_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        self.embedding=nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn= nn.LSTM(emb_dim,hid_dim,n_layers, dropout=dropout)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,src):\n",
    "        embedded=self.dropout(self.embedding(src))\n",
    "        outputs,(hidden,cell)=self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim=emb_dim\n",
    "        self.hid_dim=hid_dim\n",
    "        self.output_dim=output_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        self.embedding=nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn=nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "        self.out=nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        input=input.unsqueeze(0)\n",
    "        embedded=self.dropout(self.embedding(input))\n",
    "        output, (hidden,cell)=self.rnn(embedded,(hidden, cell))\n",
    "        prediction =self.out(output.squeeze(0))\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__ (self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder= encoder\n",
    "        self.decoder= decoder\n",
    "        self.device= device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "        \"Hidden dimension of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "        \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size= trg.shape[1]\n",
    "        max_len=trg.shape[0]\n",
    "        trg_vocab_size=self.decoder.output_dim\n",
    "        \n",
    "        outputs=torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        hidden, cell=self.encoder(src)\n",
    "        \n",
    "        input= trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell= self.decoder(input, hidden, cell)\n",
    "            outputs[t]=output\n",
    "            teacher_force= random.random()< teacher_forcing_ratio\n",
    "            top1=output.max(1)[1]\n",
    "            input=(trg[t] if teacher_force else top1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM=len(SRC.vocab)\n",
    "OUTPUT_DIM=len(TRG.vocab)\n",
    "ENC_EMB_DIM=256\n",
    "DEC_EMB_DIM=256\n",
    "HID_DIM=512\n",
    "N_LAYERS=2\n",
    "ENC_DROPOUT=0.5\n",
    "DEC_DROPOUT=0.5\n",
    "\n",
    "enc=Encoder(INPUT_DIM, ENC_EMB_DIM,HID_DIM,N_LAYERS,ENC_DROPOUT)\n",
    "dec=Decoder(INPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model= Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (out): Linear(in_features=512, out_features=7855, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 15,407,791 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX=TRG.vocab.stoi['<pad>']\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src=batch.src\n",
    "        trg=batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output=model(src,trg)\n",
    "        \n",
    "        output=output[1:].view(-1,output.shape[-1])\n",
    "        trg=trg[1:].view(-1)\n",
    "        \n",
    "        loss=criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss +=loss.item()\n",
    "        \n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            src= batch.src\n",
    "            trg= batch.trg\n",
    "            \n",
    "            output= model(src, trg, 0)\n",
    "            \n",
    "            output=output[1:].view(-1, output.shape[-1])\n",
    "            trg=trg[1:].view(-1)\n",
    "            \n",
    "            loss=criterion(output, trg)\n",
    "            \n",
    "            epoch_loss+= loss.item()\n",
    "    return epoch_loss/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time-start_time\n",
    "    elapsed_mins= int(elapsed_time/60)\n",
    "    elapsed_secs= int(elapsed_time -(elapsed_mins*60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 36s\n",
      "\tTrain Loss: 2.588 | Train PPL:  13.307\n",
      "\t Val. Loss: 3.665 |  Val. PPL:  39.045\n",
      "Epoch: 02 | Time: 0m 37s\n",
      "\tTrain Loss: 2.530 | Train PPL:  12.554\n",
      "\t Val. Loss: 3.666 |  Val. PPL:  39.079\n",
      "Epoch: 03 | Time: 0m 36s\n",
      "\tTrain Loss: 2.443 | Train PPL:  11.508\n",
      "\t Val. Loss: 3.660 |  Val. PPL:  38.845\n",
      "Epoch: 04 | Time: 0m 36s\n",
      "\tTrain Loss: 2.375 | Train PPL:  10.755\n",
      "\t Val. Loss: 3.659 |  Val. PPL:  38.837\n",
      "Epoch: 05 | Time: 0m 36s\n",
      "\tTrain Loss: 2.305 | Train PPL:  10.029\n",
      "\t Val. Loss: 3.641 |  Val. PPL:  38.119\n",
      "Epoch: 06 | Time: 0m 36s\n",
      "\tTrain Loss: 2.245 | Train PPL:   9.436\n",
      "\t Val. Loss: 3.725 |  Val. PPL:  41.469\n",
      "Epoch: 07 | Time: 0m 37s\n",
      "\tTrain Loss: 2.168 | Train PPL:   8.738\n",
      "\t Val. Loss: 3.720 |  Val. PPL:  41.264\n",
      "Epoch: 08 | Time: 0m 37s\n",
      "\tTrain Loss: 2.118 | Train PPL:   8.318\n",
      "\t Val. Loss: 3.730 |  Val. PPL:  41.684\n",
      "Epoch: 09 | Time: 0m 37s\n",
      "\tTrain Loss: 2.061 | Train PPL:   7.852\n",
      "\t Val. Loss: 3.692 |  Val. PPL:  40.133\n",
      "Epoch: 10 | Time: 0m 36s\n",
      "\tTrain Loss: 1.998 | Train PPL:   7.372\n",
      "\t Val. Loss: 3.780 |  Val. PPL:  43.836\n",
      "Epoch: 11 | Time: 0m 37s\n",
      "\tTrain Loss: 1.943 | Train PPL:   6.981\n",
      "\t Val. Loss: 3.718 |  Val. PPL:  41.165\n",
      "Epoch: 12 | Time: 0m 37s\n",
      "\tTrain Loss: 1.902 | Train PPL:   6.698\n",
      "\t Val. Loss: 3.705 |  Val. PPL:  40.643\n",
      "Epoch: 13 | Time: 0m 36s\n",
      "\tTrain Loss: 1.876 | Train PPL:   6.529\n",
      "\t Val. Loss: 3.759 |  Val. PPL:  42.916\n",
      "Epoch: 14 | Time: 0m 37s\n",
      "\tTrain Loss: 1.813 | Train PPL:   6.132\n",
      "\t Val. Loss: 3.806 |  Val. PPL:  44.949\n",
      "Epoch: 15 | Time: 0m 37s\n",
      "\tTrain Loss: 1.744 | Train PPL:   5.720\n",
      "\t Val. Loss: 3.848 |  Val. PPL:  46.882\n",
      "Epoch: 16 | Time: 0m 37s\n",
      "\tTrain Loss: 1.719 | Train PPL:   5.578\n",
      "\t Val. Loss: 3.813 |  Val. PPL:  45.279\n",
      "Epoch: 17 | Time: 0m 37s\n",
      "\tTrain Loss: 1.678 | Train PPL:   5.354\n",
      "\t Val. Loss: 3.828 |  Val. PPL:  45.959\n",
      "Epoch: 18 | Time: 0m 37s\n",
      "\tTrain Loss: 1.644 | Train PPL:   5.176\n",
      "\t Val. Loss: 3.837 |  Val. PPL:  46.373\n",
      "Epoch: 19 | Time: 0m 37s\n",
      "\tTrain Loss: 1.579 | Train PPL:   4.849\n",
      "\t Val. Loss: 3.923 |  Val. PPL:  50.556\n",
      "Epoch: 20 | Time: 0m 36s\n",
      "\tTrain Loss: 1.571 | Train PPL:   4.812\n",
      "\t Val. Loss: 3.960 |  Val. PPL:  52.468\n",
      "Epoch: 21 | Time: 0m 36s\n",
      "\tTrain Loss: 1.509 | Train PPL:   4.522\n",
      "\t Val. Loss: 3.970 |  Val. PPL:  52.977\n",
      "Epoch: 22 | Time: 0m 36s\n",
      "\tTrain Loss: 1.485 | Train PPL:   4.413\n",
      "\t Val. Loss: 3.974 |  Val. PPL:  53.223\n",
      "Epoch: 23 | Time: 0m 36s\n",
      "\tTrain Loss: 1.438 | Train PPL:   4.214\n",
      "\t Val. Loss: 3.999 |  Val. PPL:  54.568\n",
      "Epoch: 24 | Time: 0m 36s\n",
      "\tTrain Loss: 1.411 | Train PPL:   4.098\n",
      "\t Val. Loss: 4.036 |  Val. PPL:  56.615\n",
      "Epoch: 25 | Time: 0m 36s\n",
      "\tTrain Loss: 1.384 | Train PPL:   3.992\n",
      "\t Val. Loss: 3.998 |  Val. PPL:  54.501\n",
      "Epoch: 26 | Time: 0m 37s\n",
      "\tTrain Loss: 1.339 | Train PPL:   3.816\n",
      "\t Val. Loss: 4.101 |  Val. PPL:  60.409\n",
      "Epoch: 27 | Time: 0m 36s\n",
      "\tTrain Loss: 1.309 | Train PPL:   3.701\n",
      "\t Val. Loss: 4.155 |  Val. PPL:  63.753\n",
      "Epoch: 28 | Time: 0m 37s\n",
      "\tTrain Loss: 1.284 | Train PPL:   3.609\n",
      "\t Val. Loss: 4.149 |  Val. PPL:  63.388\n",
      "Epoch: 29 | Time: 0m 36s\n",
      "\tTrain Loss: 1.260 | Train PPL:   3.527\n",
      "\t Val. Loss: 4.188 |  Val. PPL:  65.869\n",
      "Epoch: 30 | Time: 0m 37s\n",
      "\tTrain Loss: 1.246 | Train PPL:   3.475\n",
      "\t Val. Loss: 4.138 |  Val. PPL:  62.687\n",
      "Epoch: 31 | Time: 0m 36s\n",
      "\tTrain Loss: 1.202 | Train PPL:   3.326\n",
      "\t Val. Loss: 4.246 |  Val. PPL:  69.792\n",
      "Epoch: 32 | Time: 0m 37s\n",
      "\tTrain Loss: 1.170 | Train PPL:   3.223\n",
      "\t Val. Loss: 4.235 |  Val. PPL:  69.089\n",
      "Epoch: 33 | Time: 0m 36s\n",
      "\tTrain Loss: 1.148 | Train PPL:   3.152\n",
      "\t Val. Loss: 4.271 |  Val. PPL:  71.584\n",
      "Epoch: 34 | Time: 0m 36s\n",
      "\tTrain Loss: 1.116 | Train PPL:   3.052\n",
      "\t Val. Loss: 4.293 |  Val. PPL:  73.202\n",
      "Epoch: 35 | Time: 0m 36s\n",
      "\tTrain Loss: 1.104 | Train PPL:   3.015\n",
      "\t Val. Loss: 4.346 |  Val. PPL:  77.174\n",
      "Epoch: 36 | Time: 0m 36s\n",
      "\tTrain Loss: 1.069 | Train PPL:   2.913\n",
      "\t Val. Loss: 4.416 |  Val. PPL:  82.726\n",
      "Epoch: 37 | Time: 0m 37s\n",
      "\tTrain Loss: 1.044 | Train PPL:   2.841\n",
      "\t Val. Loss: 4.423 |  Val. PPL:  83.328\n",
      "Epoch: 38 | Time: 0m 37s\n",
      "\tTrain Loss: 1.030 | Train PPL:   2.801\n",
      "\t Val. Loss: 4.428 |  Val. PPL:  83.754\n",
      "Epoch: 39 | Time: 0m 37s\n",
      "\tTrain Loss: 1.006 | Train PPL:   2.733\n",
      "\t Val. Loss: 4.442 |  Val. PPL:  84.929\n",
      "Epoch: 40 | Time: 0m 37s\n",
      "\tTrain Loss: 0.988 | Train PPL:   2.686\n",
      "\t Val. Loss: 4.424 |  Val. PPL:  83.398\n",
      "Epoch: 41 | Time: 0m 36s\n",
      "\tTrain Loss: 0.971 | Train PPL:   2.639\n",
      "\t Val. Loss: 4.476 |  Val. PPL:  87.868\n",
      "Epoch: 42 | Time: 0m 37s\n",
      "\tTrain Loss: 0.946 | Train PPL:   2.575\n",
      "\t Val. Loss: 4.506 |  Val. PPL:  90.593\n",
      "Epoch: 43 | Time: 0m 37s\n",
      "\tTrain Loss: 0.913 | Train PPL:   2.491\n",
      "\t Val. Loss: 4.557 |  Val. PPL:  95.304\n",
      "Epoch: 44 | Time: 0m 37s\n",
      "\tTrain Loss: 0.895 | Train PPL:   2.447\n",
      "\t Val. Loss: 4.592 |  Val. PPL:  98.721\n",
      "Epoch: 45 | Time: 0m 37s\n",
      "\tTrain Loss: 0.869 | Train PPL:   2.385\n",
      "\t Val. Loss: 4.593 |  Val. PPL:  98.771\n",
      "Epoch: 46 | Time: 0m 37s\n",
      "\tTrain Loss: 0.863 | Train PPL:   2.371\n",
      "\t Val. Loss: 4.594 |  Val. PPL:  98.867\n",
      "Epoch: 47 | Time: 0m 36s\n",
      "\tTrain Loss: 0.847 | Train PPL:   2.334\n",
      "\t Val. Loss: 4.639 |  Val. PPL: 103.395\n",
      "Epoch: 48 | Time: 0m 37s\n",
      "\tTrain Loss: 0.817 | Train PPL:   2.265\n",
      "\t Val. Loss: 4.678 |  Val. PPL: 107.560\n",
      "Epoch: 49 | Time: 0m 36s\n",
      "\tTrain Loss: 0.806 | Train PPL:   2.238\n",
      "\t Val. Loss: 4.719 |  Val. PPL: 112.104\n",
      "Epoch: 50 | Time: 0m 37s\n",
      "\tTrain Loss: 0.798 | Train PPL:   2.220\n",
      "\t Val. Loss: 4.742 |  Val. PPL: 114.683\n",
      "Epoch: 51 | Time: 0m 36s\n",
      "\tTrain Loss: 0.783 | Train PPL:   2.188\n",
      "\t Val. Loss: 4.781 |  Val. PPL: 119.198\n",
      "Epoch: 52 | Time: 0m 37s\n",
      "\tTrain Loss: 0.768 | Train PPL:   2.155\n",
      "\t Val. Loss: 4.805 |  Val. PPL: 122.086\n",
      "Epoch: 53 | Time: 0m 37s\n",
      "\tTrain Loss: 0.752 | Train PPL:   2.121\n",
      "\t Val. Loss: 4.817 |  Val. PPL: 123.655\n",
      "Epoch: 54 | Time: 0m 36s\n",
      "\tTrain Loss: 0.729 | Train PPL:   2.073\n",
      "\t Val. Loss: 4.839 |  Val. PPL: 126.323\n",
      "Epoch: 55 | Time: 0m 37s\n",
      "\tTrain Loss: 0.722 | Train PPL:   2.059\n",
      "\t Val. Loss: 4.855 |  Val. PPL: 128.387\n",
      "Epoch: 56 | Time: 0m 36s\n",
      "\tTrain Loss: 0.698 | Train PPL:   2.009\n",
      "\t Val. Loss: 4.901 |  Val. PPL: 134.432\n",
      "Epoch: 57 | Time: 0m 37s\n",
      "\tTrain Loss: 0.685 | Train PPL:   1.984\n",
      "\t Val. Loss: 4.936 |  Val. PPL: 139.196\n",
      "Epoch: 58 | Time: 0m 37s\n",
      "\tTrain Loss: 0.671 | Train PPL:   1.957\n",
      "\t Val. Loss: 4.982 |  Val. PPL: 145.698\n",
      "Epoch: 59 | Time: 0m 37s\n",
      "\tTrain Loss: 0.676 | Train PPL:   1.967\n",
      "\t Val. Loss: 4.936 |  Val. PPL: 139.159\n",
      "Epoch: 60 | Time: 0m 37s\n",
      "\tTrain Loss: 0.663 | Train PPL:   1.942\n",
      "\t Val. Loss: 4.989 |  Val. PPL: 146.853\n",
      "Epoch: 61 | Time: 0m 37s\n",
      "\tTrain Loss: 0.647 | Train PPL:   1.910\n",
      "\t Val. Loss: 4.980 |  Val. PPL: 145.524\n",
      "Epoch: 62 | Time: 0m 37s\n",
      "\tTrain Loss: 0.630 | Train PPL:   1.878\n",
      "\t Val. Loss: 5.091 |  Val. PPL: 162.611\n",
      "Epoch: 63 | Time: 0m 36s\n",
      "\tTrain Loss: 0.606 | Train PPL:   1.834\n",
      "\t Val. Loss: 5.071 |  Val. PPL: 159.336\n",
      "Epoch: 64 | Time: 0m 37s\n",
      "\tTrain Loss: 0.607 | Train PPL:   1.835\n",
      "\t Val. Loss: 5.073 |  Val. PPL: 159.597\n",
      "Epoch: 65 | Time: 0m 36s\n",
      "\tTrain Loss: 0.594 | Train PPL:   1.811\n",
      "\t Val. Loss: 5.106 |  Val. PPL: 165.021\n",
      "Epoch: 66 | Time: 0m 36s\n",
      "\tTrain Loss: 0.590 | Train PPL:   1.804\n",
      "\t Val. Loss: 5.147 |  Val. PPL: 171.840\n",
      "Epoch: 67 | Time: 0m 37s\n",
      "\tTrain Loss: 0.565 | Train PPL:   1.760\n",
      "\t Val. Loss: 5.268 |  Val. PPL: 193.972\n",
      "Epoch: 68 | Time: 0m 37s\n",
      "\tTrain Loss: 0.557 | Train PPL:   1.745\n",
      "\t Val. Loss: 5.212 |  Val. PPL: 183.526\n",
      "Epoch: 69 | Time: 0m 37s\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.730\n",
      "\t Val. Loss: 5.201 |  Val. PPL: 181.464\n",
      "Epoch: 70 | Time: 0m 37s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 5.268 |  Val. PPL: 194.011\n",
      "Epoch: 71 | Time: 0m 36s\n",
      "\tTrain Loss: 0.523 | Train PPL:   1.687\n",
      "\t Val. Loss: 5.260 |  Val. PPL: 192.388\n",
      "Epoch: 72 | Time: 0m 36s\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.703\n",
      "\t Val. Loss: 5.243 |  Val. PPL: 189.320\n",
      "Epoch: 73 | Time: 0m 37s\n",
      "\tTrain Loss: 0.507 | Train PPL:   1.660\n",
      "\t Val. Loss: 5.344 |  Val. PPL: 209.401\n",
      "Epoch: 74 | Time: 0m 37s\n",
      "\tTrain Loss: 0.507 | Train PPL:   1.660\n",
      "\t Val. Loss: 5.314 |  Val. PPL: 203.157\n",
      "Epoch: 75 | Time: 0m 36s\n",
      "\tTrain Loss: 0.491 | Train PPL:   1.633\n",
      "\t Val. Loss: 5.405 |  Val. PPL: 222.568\n",
      "Epoch: 76 | Time: 0m 37s\n",
      "\tTrain Loss: 0.494 | Train PPL:   1.639\n",
      "\t Val. Loss: 5.384 |  Val. PPL: 217.868\n",
      "Epoch: 77 | Time: 0m 36s\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.617\n",
      "\t Val. Loss: 5.382 |  Val. PPL: 217.538\n",
      "Epoch: 78 | Time: 0m 37s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.595\n",
      "\t Val. Loss: 5.442 |  Val. PPL: 230.935\n",
      "Epoch: 79 | Time: 0m 37s\n",
      "\tTrain Loss: 0.473 | Train PPL:   1.605\n",
      "\t Val. Loss: 5.409 |  Val. PPL: 223.475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Time: 0m 37s\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.571\n",
      "\t Val. Loss: 5.461 |  Val. PPL: 235.233\n",
      "Epoch: 81 | Time: 0m 37s\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.565\n",
      "\t Val. Loss: 5.529 |  Val. PPL: 251.815\n",
      "Epoch: 82 | Time: 0m 36s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
      "\t Val. Loss: 5.487 |  Val. PPL: 241.594\n",
      "Epoch: 83 | Time: 0m 36s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.545\n",
      "\t Val. Loss: 5.585 |  Val. PPL: 266.406\n",
      "Epoch: 84 | Time: 0m 37s\n",
      "\tTrain Loss: 0.436 | Train PPL:   1.546\n",
      "\t Val. Loss: 5.575 |  Val. PPL: 263.656\n",
      "Epoch: 85 | Time: 0m 36s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 5.611 |  Val. PPL: 273.371\n",
      "Epoch: 86 | Time: 0m 37s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 5.711 |  Val. PPL: 302.230\n",
      "Epoch: 87 | Time: 0m 36s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.524\n",
      "\t Val. Loss: 5.585 |  Val. PPL: 266.473\n",
      "Epoch: 88 | Time: 0m 37s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 5.644 |  Val. PPL: 282.472\n",
      "Epoch: 89 | Time: 0m 36s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.512\n",
      "\t Val. Loss: 5.660 |  Val. PPL: 287.198\n",
      "Epoch: 90 | Time: 0m 37s\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.494\n",
      "\t Val. Loss: 5.701 |  Val. PPL: 299.189\n",
      "Epoch: 91 | Time: 0m 37s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n",
      "\t Val. Loss: 5.704 |  Val. PPL: 300.017\n",
      "Epoch: 92 | Time: 0m 36s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 5.695 |  Val. PPL: 297.445\n",
      "Epoch: 93 | Time: 0m 37s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.462\n",
      "\t Val. Loss: 5.687 |  Val. PPL: 294.950\n",
      "Epoch: 94 | Time: 0m 37s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 5.790 |  Val. PPL: 327.094\n",
      "Epoch: 95 | Time: 0m 37s\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.472\n",
      "\t Val. Loss: 5.755 |  Val. PPL: 315.737\n",
      "Epoch: 96 | Time: 0m 37s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 5.776 |  Val. PPL: 322.470\n",
      "Epoch: 97 | Time: 0m 37s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 5.808 |  Val. PPL: 333.017\n",
      "Epoch: 98 | Time: 0m 37s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.424\n",
      "\t Val. Loss: 5.847 |  Val. PPL: 346.197\n",
      "Epoch: 99 | Time: 0m 37s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 5.828 |  Val. PPL: 339.639\n",
      "Epoch: 100 | Time: 0m 37s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 5.912 |  Val. PPL: 369.506\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS=100\n",
    "CLIP=1\n",
    "best_valid_loss=float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time=time.time()\n",
    "    train_loss=train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss=evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time=time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs= epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss< best_valid_loss:\n",
    "        best_valid_loss=valid_loss\n",
    "        torch.save(model.state_dict(),'../tut1-model.pt')\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../tut1-model.pt'))\n",
    "test_loss =evaluate(model, test_iterator,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6609056293964386"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
